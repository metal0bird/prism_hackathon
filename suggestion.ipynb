{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def store_interaction(timestamp, command, device):\n",
    "  conn = sqlite3.connect(\"user_interactions.db\")\n",
    "  cursor = conn.cursor()\n",
    "  cursor.execute(\"INSERT INTO interactions (timestamp, command, device) VALUES (?, ?, ?)\", (timestamp, command, device))\n",
    "  conn.commit()\n",
    "  conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_recommendation_model(data):\n",
    "  # Preprocess data (clean, format)\n",
    "  model = defaultdict(int)\n",
    "  for interaction in data:\n",
    "    prev_command, curr_command = None, interaction[\"command\"]\n",
    "    for item in data:\n",
    "      if item[\"timestamp\"] < interaction[\"timestamp\"]:\n",
    "        prev_command = item[\"command\"]\n",
    "        break\n",
    "    if prev_command:\n",
    "      model[(prev_command, curr_command)] += 1\n",
    "  return model\n",
    "\n",
    "def get_recommendation(current_command, model):\n",
    "  # Find the most likely next command based on weights in the model\n",
    "  recommendations = sorted(model.items(), key=lambda x: x[1], reverse=True)\n",
    "  if recommendations:\n",
    "    return recommendations[0][1]  # Return the second element (next command)\n",
    "  else:\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat assistant loop\n",
    "while True:\n",
    "  user_input = get_user_input()\n",
    "  # Process user input and control devices\n",
    "  store_interaction(current_timestamp, user_input, target_device)\n",
    "  recommendation = get_recommendation(user_input, recommendation_model)\n",
    "  if recommendation:\n",
    "    print(\"Would you also like to:\", recommendation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"Morning\", \"Evening\", \"Weekday\", \"Weekend\", \"LightsOn\", \"ThermostatSet\"]\n",
    "actions = [\"TurnOnLights\", \"TurnOffLights\", \"AdjustThermostat\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transition probability matrix with zeros\n",
    "transition_probabilities = [[0.0 for _ in states] for _ in states]\n",
    "\n",
    "def update_transition_probabilities(data):\n",
    "  # Iterate through interaction data\n",
    "  for interaction in data:\n",
    "    prev_state = get_state(interaction)  # Function to determine state based on data\n",
    "    curr_state = get_state(get_next_interaction(interaction, data))  # Function to find next interaction\n",
    "    transition_probabilities[states.index(prev_state)][states.index(curr_state)] += 1\n",
    "  # Normalize probabilities for each row (state)\n",
    "  for i in range(len(transition_probabilities)):\n",
    "    total = sum(transition_probabilities[i])\n",
    "    if total > 0:\n",
    "      transition_probabilities[i] = [p / total for p in transition_probabilities[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state, action):\n",
    "  # Define reward logic based on your desired user experience\n",
    "  if state == \"Evening\" and action == \"TurnOnLights\":\n",
    "    return 1.0\n",
    "  else:\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class SmartThingsEnv(gym.Env):\n",
    "  def __init__(self):\n",
    "    self.state = None\n",
    "    # ... (other environment initialization)\n",
    "\n",
    "  def step(self, action):\n",
    "    # Control SmartThings devices based on action\n",
    "    # Update state based on user interaction and environment response\n",
    "    reward = get_reward(self.state, action)\n",
    "    # ... (other environment logic)\n",
    "    return self.state, reward, done, info\n",
    "\n",
    "  def reset(self):\n",
    "    # Reset environment and user state\n",
    "    self.state = get_initial_state()  # Function to determine initial state\n",
    "    return self.state\n",
    "\n",
    "# Create the MDP environment\n",
    "env = SmartThingsEnv()\n",
    "\n",
    "# Use an MDP solver like Value Iteration or Q-Learning to learn the optimal policy\n",
    "# (code for solving the MDP omitted for brevity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mdp_recommendation(state, policy):\n",
    "  # Utilize the MDP policy to get the recommended action for the current state\n",
    "  return policy[states.index(state)]\n",
    "\n",
    "# Chat assistant loop (modified)\n",
    "while True:\n",
    "  user_input = get_user_input()\n",
    "  # Process user input and control devices\n",
    "  store_interaction(current_timestamp, user_input, target_device)\n",
    "  current_state = get_state(interaction_data)  # Update state based on interaction\n",
    "  recommendation = get_mdp_recommendation(current_state, mdp_policy)\n",
    "  if recommendation:\n",
    "    print(\"Would you also like to:\", recommendation)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
